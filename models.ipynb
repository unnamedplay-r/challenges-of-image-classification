{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tpot import TPOTClassifier\n",
    "\n",
    "# model selections\n",
    "from sklearn.svm import LinearSVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# preprocessing steps\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score\n",
    "\n",
    "# classics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import pickle\n",
    "from pprint import pprint \n",
    "\n",
    "rseed = 4444\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pxl = pd.read_pickle('df_pxl_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hog = pd.read_pickle('df_hog_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sift = pd.read_pickle('./pkls/df_sift_features_500.pkl')\n",
    "df_sift = df_sift.drop('indx', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets= ['ak47', 'american-flag', 'backpack', 'baseball-bat',\n",
    "          'baseball-glove', 'basketball-hoop', 'bat', 'bathtub', 'bear',\n",
    "          'beer-mug', 'billiards', 'binoculars', 'birdbath', 'blimp',\n",
    "          'bonsai', 'boom-box', 'bowling-ball', 'bowling-pin', 'boxing-glove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pxl = df_pxl[df_pxl['category'].isin(targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hog = df_hog[df_hog['category'].isin(targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oddly, there are a few more images in hog, than in the original df.\n",
    "# I don't undertand that.. but I don't think we're off on our data.\n",
    "df_hog.category.value_counts()[df.category.value_counts().sort_index() != df_hog.category.value_counts().sort_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('PXL Categories')\n",
    "df.category.value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('HOG Categories')\n",
    "df_hog.category.value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(df.iloc[0,1:].astype('uint8').reshape(32,32), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing and Splitting Data for Model\n",
    "\n",
    "Stratified data using custom `Ntrain_test_split` function. Was considering using in sklearn's  [stratify option](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) within the TTS function or using the [StratifuedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Ntrain_test_split(X, y, Ntrain=40, Ntest=10, dtype='float', random_seed=rseed, exclude=None):\n",
    "    \"\"\"Grabs N training samples for each category in the training data\n",
    "    \n",
    "    X - features to split\n",
    "    y - predictors to split (these must be categorical)\n",
    "    Ntrain - number of training samples you want from each category\n",
    "    Ntest - number of test samples you want from each category\n",
    "    random_seed - random seed applied to the sampling\n",
    "    exclude - list of categories to be excluded from the TTS\n",
    "    \n",
    "    \"\"\"\n",
    "    # set a seed if needed\n",
    "    if random_seed:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    unique = y.unique()\n",
    "#     print(unique)\n",
    "    if exclude:\n",
    "        # remove all labels to exclude\n",
    "        index = np.argwhere(np.in1d(unique, exclude))\n",
    "        unique = np.delete(unique, index)\n",
    "    \n",
    "    # prepare indexes\n",
    "    train = np.zeros(len(unique) * Ntrain, dtype)\n",
    "    test  = np.zeros(len(unique) * Ntest, dtype)\n",
    "    \n",
    "    # tracks lower and higher bounds of training and test arrays\n",
    "    trlb = 0\n",
    "    trhb = Ntrain\n",
    "    telb = 0\n",
    "    tehb = Ntest\n",
    "    \n",
    "    for cat in unique:\n",
    "#         print('\\nCategory :', cat)\n",
    "        # randomly sample N indicies\n",
    "        train_test = np.random.choice(y[y == cat].index, Ntrain+Ntest, replace=False)\n",
    "#         print('Sampled set', train_test)\n",
    "        \n",
    "        # use the first m indicies for the training sample, and use the rest for the test\n",
    "        # save them into the training and test index storage\n",
    "        train[trlb:trhb] = train_test[:Ntrain]\n",
    "        test[telb:tehb] = train_test[Ntrain:]\n",
    "        \n",
    "#         print('Training set :', train)\n",
    "#         print('Test set :', test)\n",
    "        \n",
    "        # increment the bound trackers\n",
    "        trlb, trhb, telb, tehb = trlb+Ntrain, trhb+Ntrain, telb+Ntest, tehb+Ntest\n",
    "        \n",
    "#     print(train)\n",
    "    X_train = X.loc[train,:]\n",
    "    y_train = y.loc[train]\n",
    "    X_test =  X.loc[test,:]\n",
    "    y_test =  y.loc[test]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building training test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # delete any data if needed\n",
    "# del y_test\n",
    "# del y_train\n",
    "# del X_test\n",
    "# del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = Ntrain_test_split(df_pxl.iloc[:,1:], df_pxl.category,\n",
    "                                                     Ntrain = 40, Ntest = 10,\n",
    "                                                     random_seed=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hX_train, hy_train, hX_test, hy_test = Ntrain_test_split(df_hog.iloc[:,1:], df_hog.category, \n",
    "                                                         Ntrain = 40, Ntest = 10,\n",
    "                                                         random_seed=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sX_train, sy_train, sX_test, sy_test = Ntrain_test_split(df_sift.iloc[:,1:], df_sift.label, \n",
    "                                                         Ntrain = 40, Ntest = 10,\n",
    "                                                         random_seed=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another possibility ..\n",
    "X_train_tts, X_test_tts, y_train_tts, y_test_tts = train_test_split(X_df, y_df, test_size=0.4, random_state=rseed, stratify=y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_mclut, y_train_mclut, X_test_mclut, y_test_mclut = Ntrain_test_split(X_df, y_df, \n",
    "                                                                             random_seed=rseed, \n",
    "                                                                             exclude=['clutter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sm, y_train_sm, X_test_sm, y_test_sm = Ntrain_test_split(X_df, y_df, \n",
    "                                                                 Ntrain=10, Ntest=2,\n",
    "                                                                 random_seed=rseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing their shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_train_mclut.shape, X_train_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_test.shape, X_test_mclut.shape, X_test_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_test.shape, y_test_mclut.shape, y_test_sm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do this if you need to ensure that the category variables line up from the test split\n",
    "# y_test_copy = pd.DataFrame(y_df.loc[y_test.index])\n",
    "# y_test_copy['cat_test'] = y_test\n",
    "# y_test_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classifier\n",
    "\n",
    "#### TODO:\n",
    "1. Build visualizations\n",
    "    - ROC curves\n",
    "    - Confusion Matrix\n",
    "2. Discover metrics for this\n",
    "3. Do grid search on parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accuracy: 0.40`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Standardize our data around meanCreate a SVC classifier using a linear kernel\n",
    "pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                       ('clf', LinearSVC(random_state=0))])\n",
    "%time pipe_lrSVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accuracy: 0.05`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Standardize our data around mean. Create a SVC classifier using a linear kernel and SGD\n",
    "n_iter = np.ceil(10**6 / hX_train.shape[0])\n",
    "pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                       ('clf', SGDClassifier(loss='squared_hinge', \n",
    "                                             penalty='l2',\n",
    "                                             alpha=0.001,\n",
    "                                             random_state=rseed,\n",
    "                                             n_iter = 20))])\n",
    "%time pipe_lrSVC.fit(hX_train, hy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_lrSVC.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                       ('clf', SGDClassifier(loss='squared_hinge', \n",
    "                                             penalty='l2',\n",
    "                                             alpha=0.001,\n",
    "                                             random_state=rseed))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_train = np.random.choice(X_train.index, len(X_train), replace=False)\n",
    "num = []\n",
    "\n",
    "pipe_lrSVC.named_steps['scaler'].fit(X_train)\n",
    "\n",
    "batchsize = 100\n",
    "for ind in range(0,len(r_train),batchsize):\n",
    "    partial_pipe_fit(pipe_lrSVC, X_train.loc[r_train[ind:ind+batchsize], :], y_train.loc[r_train[ind:ind+batchsize]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_pipe_fit(pipeline_obj, X,y):\n",
    "    X = pipeline_obj.named_steps['scaler'].transform(X)\n",
    "    pipeline_obj.named_steps['clf'].partial_fit(X,y, classes=y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_lrSVC.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PIXEL FEATURES\n",
    "\n",
    "# with open('pipe_lrSVC.pkl', 'wb') as f:\n",
    "#     pickle.dump(pipe_lrSVC, f)\n",
    "\n",
    "with open('pxl_lrSVC.pkl', 'rb') as f:\n",
    "    pxl_lrSVC = pickle.load(f)\n",
    "\n",
    "\n",
    "# HOG FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pxl_lrSVC.named_steps['clf'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pxl_lrSVC.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_actual = pipe_lrSVC.predict(X_test)\n",
    "# y_pred_mclut = pipe_lrSVC.predict(X_test_mclut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame({'actual' : y_test,\n",
    "#               'prediction': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix without clutter category\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_actual)\n",
    "cnf_matrix_mclut = confusion_matrix(y_test_mclut, y_pred_mclut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix, interpolation='nearest', cmap=diverging_cmap)\n",
    "plt.title('Linear SVM')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cnf_matrix_mclut, interpolation='nearest', cmap=diverging_cmap)\n",
    "plt.title('Linear SVM')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifierm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PXL Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN & SVM  \n",
    "Misclassifications  \n",
    "Showcasing  \n",
    "HEATMAP The hog  \n",
    "Neural net - less data to train on a convolutional neural net, smaller model, internet of things  \n",
    "Business case - grocery store (something that runs on handscanners), getting new items from a grocery store  \n",
    " - price checks on grocery stores can improve as bar codes can fade from materials\n",
    "\n",
    "Logo recognition on clothing (direct consumer marketing)  \n",
    "Accuracy as a function of categories  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Standardize our data around meanCreate a SVC classifier using a linear kernel\n",
    "pxl_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                           ('clf', LinearSVC(random_state=0))])\n",
    "%time pxl_pipe_lrSVC.fit(X_train, y_train)\n",
    "y_pred_pxl = pxl_pipe_lrSVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy:', accuracy_score(y_test, y_pred_pxl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprint(classification_report(y_test, y_pred_pxl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "pxl_cnf_matrix = confusion_matrix(y_test, y_pred_pxl)\n",
    "df_pxl_cm = pd.merge(pd.DataFrame(y_train.unique(), columns=['category']),\n",
    "                     pd.DataFrame(pxl_cnf_matrix), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(pxl_cnf_matrix, interpolation='nearest', cmap=diverging_cmap)\n",
    "plt.title('LinearSVM Pixel Features', fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "plt.colorbar(drawedges=True,\n",
    "            spacing='proportional',\n",
    "            ticks=range(0,11),\n",
    "            shrink=.82)\n",
    "plt.ylabel('True label (Category #)')\n",
    "plt.yticks(range(0,19), df_pxl_cm.category)\n",
    "\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.xticks(range(0,19), range(1,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "worst_performers = df_pxl_cm.loc[df_pxl_cm.max(axis=1).sort_values()[:N].index,:].sort_index()\n",
    "\n",
    "# sns.set_context('poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(worst_performers.iloc[:,1:], interpolation='nearest',aspect='auto', cmap=diverging_cmap)\n",
    "plt.title('Worst Performers Using Pixel Features')\n",
    "plt.grid(False)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "# plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.yticks(range(0,N), worst_performers.category);\n",
    "plt.tick_params(axis='y', which='both', labelsize=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "best_performers = df_pxl_cm.loc[df_pxl_cm.max(axis=1).sort_values()[-N:].index,:].sort_index()\n",
    "\n",
    "# sns.set_context('poster')\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(best_performers.iloc[:,1:], interpolation='nearest',aspect='auto', cmap=diverging_cmap)\n",
    "plt.title('Best Performers Using Pixel Features')\n",
    "plt.grid(False)\n",
    "# plt.colorbar()\n",
    "plt.tight_layout()\n",
    "# plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.yticks(range(0,N), best_performers.category);\n",
    "plt.tick_params(axis='y', which='both', labelsize=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HOG FEATURES\n",
    "\n",
    "# with open('hog_pipe_lrSVC.pkl', 'wb') as f:\n",
    "#     pickle.dump(hog_pipe_lrSVC, f)\n",
    "\n",
    "# with open('hog_pxl_lrSVC.pkl', 'rb') as f:\n",
    "#     hog_pipe_lrSVC = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.6 s, sys: 58.4 ms, total: 8.66 s\n",
      "Wall time: 8.7 s\n",
      "Accuracy: 0.236842105263\n"
     ]
    }
   ],
   "source": [
    "# # Standardize our data around meanCreate a SVC classifier using a linear kernel\n",
    "hog_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                           ('clf', LinearSVC(random_state=0))])\n",
    "%time hog_pipe_lrSVC.fit(hX_train, hy_train)\n",
    "print('Accuracy:', hog_pipe_lrSVC.score(hX_test, hy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hog_pred = hog_pipe_lrSVC.predict(hX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('                 precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           ak47       0.22      0.20      0.21        10\\n'\n",
      " '  american-flag       0.00      0.00      0.00        10\\n'\n",
      " '       backpack       0.60      0.30      0.40        10\\n'\n",
      " '   baseball-bat       0.40      0.40      0.40        10\\n'\n",
      " ' baseball-glove       0.25      0.20      0.22        10\\n'\n",
      " 'basketball-hoop       0.17      0.10      0.12        10\\n'\n",
      " '            bat       0.17      0.20      0.18        10\\n'\n",
      " '        bathtub       0.17      0.20      0.18        10\\n'\n",
      " '           bear       0.07      0.20      0.11        10\\n'\n",
      " '       beer-mug       0.55      0.60      0.57        10\\n'\n",
      " '      billiards       0.25      0.20      0.22        10\\n'\n",
      " '     binoculars       0.75      0.30      0.43        10\\n'\n",
      " '       birdbath       0.11      0.30      0.16        10\\n'\n",
      " '          blimp       0.08      0.10      0.09        10\\n'\n",
      " '         bonsai       0.23      0.30      0.26        10\\n'\n",
      " '       boom-box       0.25      0.10      0.14        10\\n'\n",
      " '   bowling-ball       0.71      0.50      0.59        10\\n'\n",
      " '    bowling-pin       0.33      0.30      0.32        10\\n'\n",
      " '   boxing-glove       0.00      0.00      0.00        10\\n'\n",
      " '\\n'\n",
      " '    avg / total       0.28      0.24      0.24       190\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(classification_report(hy_test, hog_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(1024, random_state=rseed)\n",
    "# kmeans_217 = MiniBatchKMeans(217, random_state=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_select = np.random.choice(hX_train.index, size=10280, replace=False)\n",
    "batch_size = 2000\n",
    "for ind in range(0,hX_train.shape[0],batch_size):\n",
    "    print(ind)\n",
    "    kmeans.partial_fit(hX_train.loc[rand_select[ind:ind+batch_size], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ind in range(0,hX_train.shape[0],batch_size):\n",
    "    print(ind)\n",
    "    kmeans_217.partial_fit(hX_train.loc[rand_select[ind:ind+batch_size], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "khX_train = kmeans.transform(hX_train)\n",
    "# k217hX_train = kmeans_217.transform(hX_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Standardize our data around meanCreate a SVC classifier using a linear kernel\n",
    "hog_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                           ('clf', LinearSVC(random_state=0))])\n",
    "%time hog_pipe_lrSVC.fit(khX_train, hy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # Standardize our data around meanCreate a SVC classifier using a linear kernel\n",
    "# hog217_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "#                               ('clf', LinearSVC(random_state=0))])\n",
    "# %time hog217_pipe_lrSVC.fit(k217hX_train, hy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "khX_test = kmeans.transform(hX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "khog_pred = hog_pipe_lrSVC.predict(khX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time hog_pipe_lrSVC.score(khX_test, hy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparseCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaledClusters = std_scaler.fit_transform(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scdr = SparseCoder(scaledClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse_matrix = scdr.transform(hX_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# diverging_cmap = matplotlib.colors.ListedColormap(sns.cubehelix_palette(8).as_hex())\n",
    "# Altering the cubehelix: \n",
    "# http://seaborn.pydata.org/generated/seaborn.cubehelix_palette.html#seaborn.cubehelix_palette\n",
    "# See here for where cmap comes from:\n",
    "# http://stackoverflow.com/questions/37902459/how-do-i-use-seaborns-color-palette-as-a-colormap-in-matplotlib\n",
    "diverging_cmap = sns.cubehelix_palette(start=2,\n",
    "                                       rot=0.2,\n",
    "                                       gamma=1.5,\n",
    "                                       hue=1, \n",
    "                                       light=1, \n",
    "                                       dark=0, \n",
    "                                       as_cmap = True)\n",
    "\n",
    "diverging_cube = sns.cubehelix_palette(n_colors=257,\n",
    "                                       start=2,\n",
    "                                       rot=0.2,\n",
    "                                       gamma=1.5,\n",
    "                                       hue=1, \n",
    "                                       light=0.9, \n",
    "                                       dark=0.3,\n",
    "                                       reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "hog_cnf_matrix = confusion_matrix(hy_test, hog_pred)\n",
    "df_hog_cm = pd.merge(pd.DataFrame(hy_test.unique(), columns=['category']),\n",
    "                     pd.DataFrame(hog_cnf_matrix), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(hog_cnf_matrix, interpolation='nearest', cmap=diverging_cmap)\n",
    "plt.title('LinearSVM HOG Features', fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "plt.colorbar(drawedges=True,\n",
    "            spacing='proportional',\n",
    "            ticks=range(0,11),\n",
    "            shrink=.82)\n",
    "plt.ylabel('True label (Category #)')\n",
    "plt.yticks(range(0,19), df_hog_cm.category)\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.xticks(range(0,19), range(1,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "worst_performers = df_hog_cm.loc[df_hog_cm.max(axis=1).sort_values()[:N].index,:].sort_index()\n",
    "\n",
    "# sns.set_context('poster')\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.imshow(worst_performers.iloc[:,1:], interpolation='nearest',aspect='auto', cmap=diverging_cmap)\n",
    "plt.title('Worst Performers using HOG Features', fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "plt.colorbar(drawedges=True,\n",
    "            spacing='proportional',\n",
    "            ticks=range(0,11),\n",
    "            shrink=.82)\n",
    "plt.tight_layout()\n",
    "# plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.yticks(range(0,N), worst_performers.category);\n",
    "plt.tick_params(axis='y', which='both', labelsize=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "best_performers = df_hog_cm.loc[df_hog_cm.max(axis=1).sort_values()[-N:].index,:].sort_index()\n",
    "\n",
    "# sns.set_context('poster')\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.imshow(best_performers.iloc[:,1:], interpolation='nearest',aspect='auto', cmap=diverging_cmap)\n",
    "plt.title('Best Performers Using HOG Features', fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "plt.colorbar(drawedges=True,\n",
    "            spacing='proportional',\n",
    "            ticks=range(0,11),\n",
    "            shrink=.82)\n",
    "plt.tight_layout()\n",
    "# plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.yticks(range(0,N), best_performers.category);\n",
    "plt.tick_params(axis='y', which='both', labelsize=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SIFT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import sparse_encode\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary\n",
    "dimensions = 37\n",
    "mbdl = MiniBatchDictionaryLearning(dimensions, split_sign=True, random_state=rseed)\n",
    "mbdl.fit(sX_train)\n",
    "\n",
    "# Sparse Encoder\n",
    "code = sparse_encode(sX_train, mbdl.components_)\n",
    "\n",
    "# # Standardize our data around meanCreate a SVC classifier using a linear kernel\n",
    "sift_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                            ('clf', LinearSVC(random_state=0))])\n",
    "%time sift_pipe_lrSVC.fit(code, sy_train)\n",
    "sX_test_sp = sparse_encode(sX_test, mbdl.components_)\n",
    "sift_pred = sift_pipe_lrSVC.predict(sX_test_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprint(classification_report(sy_test, sift_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy:', sift_pipe_lrSVC.score(sX_test_sp, sy_test))\n",
    "# print('F1-Score:', f1_score(sy_test, sX_test_sp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sy_test.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "sift_cnf_matrix = confusion_matrix(sy_test, sift_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sift_cm = pd.merge(pd.DataFrame(sy_test.unique(), columns=['category']),\n",
    "                     pd.DataFrame(sift_cnf_matrix), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(sift_cnf_matrix, interpolation='nearest', cmap=diverging_cmap)\n",
    "plt.title('LinearSVM SIFT Features',  fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "plt.colorbar(drawedges=True,\n",
    "            spacing='proportional',\n",
    "            ticks=range(0,11),\n",
    "            shrink=.82)\n",
    "plt.xticks(range(0,19), range(0,20))\n",
    "plt.yticks(range(0,19), df_sift_cm.category)\n",
    "\n",
    "plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "worst_performers = df_sift_cm.loc[df_sift_cm.max(axis=1).sort_values()[:N].index,:].sort_index()\n",
    "\n",
    "# sns.set_context('poster')\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.imshow(worst_performers.iloc[:,1:], interpolation='nearest',aspect='auto', cmap=diverging_cmap)\n",
    "plt.title('Worst Performers using SIFT Features', fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "# plt.colorbar()\n",
    "plt.tight_layout()\n",
    "# plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.yticks(range(0,N), worst_performers.category);\n",
    "plt.tick_params(axis='y', which='both', labelsize=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "best_performers = df_sift_cm.loc[df_sift_cm.max(axis=1).sort_values()[-N:].index,:].sort_index()\n",
    "\n",
    "# sns.set_context('poster')\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.imshow(best_performers.iloc[:,1:], interpolation='nearest',aspect='auto', cmap=diverging_cmap)\n",
    "plt.title('Best Performers Using SIFT Features', fontsize=25, y=1.02)\n",
    "plt.grid(False)\n",
    "plt.colorbar(drawedges=True,\n",
    "            spacing='proportional',\n",
    "            ticks=range(0,11),\n",
    "            shrink=.82)\n",
    "plt.tight_layout()\n",
    "# plt.ylabel('True label (Category #)')\n",
    "plt.xlabel('Predicted label (Category #)');\n",
    "plt.yticks(range(0,N), best_performers.category);\n",
    "plt.tick_params(axis='y', which='both', labelsize=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights plot? \n",
    "# sns.heatmap(sift_pipe_lrSVC.steps[1][-1].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments on Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dimensions in range(80,150):\n",
    "    # Dictionary\n",
    "    mbdl = MiniBatchDictionaryLearning(dimensions, split_sign=True, random_state=rseed)\n",
    "    mbdl.fit(sX_train)\n",
    "\n",
    "    # Sparse Encoder\n",
    "    code = sparse_encode(sX_train, mbdl.components_)\n",
    "\n",
    "    # Standardize our data around mean & Create a SVC classifier using a linear kernel\n",
    "    sift_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                                ('clf', LinearSVC(random_state=0))])\n",
    "    sift_pipe_lrSVC.fit(code, sy_train)\n",
    "    \n",
    "    acc.append(sift_pipe_lrSVC.score(sparse_encode(sX_test, mbdl.components_), sy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc.index(max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary\n",
    "mbdl = MiniBatchDictionaryLearning(37, split_sign=True, random_state=rseed)\n",
    "mbdl.fit(sX_train)\n",
    "\n",
    "# Sparse Encoder\n",
    "code = sparse_encode(sX_train, mbdl.components_)\n",
    "\n",
    "# Standardize our data around mean & Create a SVC classifier using a linear kernel\n",
    "sift_pipe_lrSVC = Pipeline([('scaler', StandardScaler()),\n",
    "                            ('clf', LinearSVC(random_state=0))])\n",
    "sift_pipe_lrSVC.fit(code, sy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sX_test_sparse = sparse_encode(sX_test, mbdl.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(acc);\n",
    "sns.despine()\n",
    "plt.title('# of Visual Words vs. Accuracy', fontsize=25, y=1.02)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('# of Visual Words')\n",
    "plt.xlim((-0.2,125))\n",
    "plt.xticks(range(0,130,10));\n",
    "# plt.xticks()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dict_acc', 'wb') as f:\n",
    "    pickle.dump(acc, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
